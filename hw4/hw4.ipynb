{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JC5iBT_0OCSl",
        "OA975wHjOJI-",
        "mORb9IwPP7oE",
        "GqbGdV6oXEyP",
        "2zpnxENIWj4N",
        "MX2EqMuqOskz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt44vyY1xW9b",
        "colab_type": "text"
      },
      "source": [
        "# 0. General Concepts\n",
        "## artificial intelligence, machine learning, deep learning\n",
        "In this course we covered much of the theory and math behind some of the most common Artificial Intelligence concepts during our lectures.\n",
        "\n",
        "We spoke about the distinctions between terms like artificial intelligence, machine learning, and deep learning. We discussed unsupervised, supervised and reinforcement learning; three areas in machine learning.\n",
        "\n",
        "Artificial intelligence is a sort of 'umbrella' term we use to describe a wide array of concepts, ideas, and practices. This includes everything from the FSMs behind the Ghosts in Pacman to DeepMind's AlphaGo and everything in between.\n",
        "\n",
        "Machine Learning is an area of study within the domain of artificial intelligence. Nested one layer deeper is deep learning: a class of machine learning algorithms.\n",
        "\n",
        "The focus of the course stayed on machine learning as we went in depth into topics like linear regression, gradient descent, and many others. As we dove further into building neural networks, we were introduced to higher-level, machine learning focused software APIs like tensorflow and keras. \n",
        "\n",
        "Our three homework projects prior to this focused primarily on computer vision concepts and ideas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJyddqGexl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA975wHjOJI-",
        "colab_type": "text"
      },
      "source": [
        "# 1. Building a model\n",
        "## structure of a convent, components\n",
        "\n",
        "Below I've utilized some code from a previous assignment to help demonstrate the process of building a machine learning model.\n",
        "\n",
        "Every machine learning learning model has components. In the example below lines 2-10 show us adding dense and dropout layers using the keras API as we build a convolutional neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX6SGTpMOIKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the layers\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "    keras.layers.Dropout(0.1, noise_shape=None, seed=None),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dropout(0.2, noise_shape=None, seed=None),\n",
        "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "    keras.layers.Dropout(0.4, noise_shape=None, seed=None),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmtUU5X0jiCV",
        "colab_type": "text"
      },
      "source": [
        "# 2. Compiling a Model\n",
        "\n",
        "After the structure of the model has been defined we now need to compile the model. Keras gives some parameters to help customize our model via the compile function. \n",
        "\n",
        "Here we choose 'adam' as our optimizer however, we could just as easily have chosen others such as 'SGD'(for $S$tochastic $G$radient $D$escent). Optimizers are chosen to minimize or maximize the cost function of your particular model.\n",
        "\n",
        "We can also specify the learning rate. Which can be thought of as how much the 'weights' are updated during training and in the range of [0.0 - 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Umfz9sjhkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI5T_zI1jSuC",
        "colab_type": "text"
      },
      "source": [
        "# 3. Training a Model\n",
        "\n",
        "Here we begin the process of training our model. We pass in the training data and their associated labels, the desired number of epochs, and the test data and labels as validation data.\n",
        "\n",
        "The outcome of a trained model can be one of three cases: overfit, underfit, or good fit. To underfit is to have our test predictions nearly indistinguishable from our model function. To have our model overfit is to have our model effectively 'memorize' the training data, including all the noise!\n",
        "\n",
        "We must first train the model so that we can use our trained model to make predictions on new data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOmdE-6GjP6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "fb50a2f8-c9ce-4b66-e1ce-037031fbd429"
      },
      "source": [
        "# train the model\n",
        "epochs = 10\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5840 - acc: 0.7881 - val_loss: 0.4312 - val_acc: 0.8396\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4310 - acc: 0.8437 - val_loss: 0.4077 - val_acc: 0.8512\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.3973 - acc: 0.8538 - val_loss: 0.3969 - val_acc: 0.8538\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.3762 - acc: 0.8623 - val_loss: 0.3776 - val_acc: 0.8642\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.3595 - acc: 0.8673 - val_loss: 0.3818 - val_acc: 0.8683\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.3462 - acc: 0.8721 - val_loss: 0.3734 - val_acc: 0.8631\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.3423 - acc: 0.8749 - val_loss: 0.3619 - val_acc: 0.8697\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.3310 - acc: 0.8801 - val_loss: 0.3602 - val_acc: 0.8698\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.3224 - acc: 0.8823 - val_loss: 0.3464 - val_acc: 0.8753\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.3216 - acc: 0.8815 - val_loss: 0.3520 - val_acc: 0.8758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORb9IwPP7oE",
        "colab_type": "text"
      },
      "source": [
        "# 4. Finetuning a Pretrained Model\n",
        "\n",
        "There are a few steps we can take to fine tune a pretrained model. One thing we could do would be to play around with the number, type, and order of layers in our model. For example, we could add dropout layers or more dense layers or even reorder the existing layers all in an effort to fine tune. \n",
        "\n",
        "We could also make adjustments to variables such as how many steps per epoch, the batch size etc."
      ]
    }
  ]
}